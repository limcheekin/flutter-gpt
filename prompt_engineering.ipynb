{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QqY6ps7Uyfa7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/prompt-engineering.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/prompt-engineering.ipynb)\n",
        "\n",
        "# Prompt Engineering\n",
        "\n",
        "In this notebook we'll explore the fundamentals of prompt engineering. We'll start by installing the relevant libraries using `setup.sh`, which we'll be using throughout these examples. However, note that we can use other LLMs here, like those offered by Cohere or open source alternatives available via Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O98uvEv3yfbE"
      },
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WonTubBfyfbG"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXuuagtyfbI"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a FLAN-T5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "De2sRootyfbJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading model...\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "print('loading model...')\n",
        "\n",
        "try:\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"flan-t5-small/\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"flan-t5-small/\")\n",
        "except:\n",
        "    print(\"An exception occurred on loading model.\")\n",
        "    \n",
        "def completion(\n",
        "        prompt: str = None, \n",
        "        min_length: int = 20, \n",
        "        max_length: int = 200, \n",
        "        temperature: float = 0.0\n",
        "    ):    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "            **inputs,\n",
        "            min_length=min_length,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=1.0,\n",
        "            top_k=50)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5UABsQyfbJ"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "otCz3qA-yfbK",
        "outputId": "036faddf-b14d-4531-aa5b-8b1acd6c4cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\"]\n"
          ]
        }
      ],
      "source": [
        "res = completion(prompt)\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6HQ2lUTyfbM"
      },
      "source": [
        "Alternatively, if we do have the correct information withing the `context`, the model should reply with `\"I don't know\"`, let's try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "u7baNoptyfbN",
        "outputId": "6438c9ee-cc73-4413-f9e4-8abd5d7cf023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"I don't know. Is there a library or model provider that offers LLMs?\"]\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Libraries are places full of books.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = completion(prompt)\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2C0_Ta7yfbO"
      },
      "source": [
        "Perfect, our instructions are being understood by the model. In most real use-cases we won't be providing the external information / context to the model manually. Instead, it will be an automatic process using something like [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) to retrieve relevant information from an external source.\n",
        "\n",
        "For now, that's beyond the scope of what we're exploring here, you can find more on that in the link above.\n",
        "\n",
        "In summary, a prompt often consists of those four components: instructions, context(s), user input, and the output indicator. Now we'll take a look at creative vs. stricter generation.\n",
        "\n",
        "## Generation Temperature\n",
        "\n",
        "The `temperature` parameter used in generation models tells us how \"random\" the model can be. It represents the probability of a model to choose a word which is *not* the first choice of the model.\n",
        "\n",
        "This works because the model is actually assigning a probability prediction across all tokens within it's vocabulary with each _\"step\"_ of the model (each new word or sub-word).\n",
        "\n",
        "TK visual demonstrating steps over tokens\n",
        "\n",
        "With each new step forwards the model considers the previous tokens fed into the model, creates an embedding by encoding the information from these tokens over many model encoder layers, then passes this encoding to a decoder. The decoder then predicts the probability of each token that the model knows (ie is within the model *vocabulary*) based on the information encoded within the embedding.\n",
        "\n",
        "TK visualize the encoding at one timestep -> decoding -> prediction over many tokens\n",
        "\n",
        "At a temperature of `0.0` the decoder will always select the top predicted token. At a temperature of `1.0` the model will always select a word that *is predicted* considering it's assigned probability.\n",
        "\n",
        "TK visualize selection of always top word over several timesteps when temp == 0, compared to selection of over words over several timesteps when temp == 1\n",
        "\n",
        "Considering all of this, if we have a conservative, fact based Q&A like in the previous example, it makes sense to set a lower `temperature`. However, if we're wanting to produce some creative writing or chatbot conversations, we might want to experiment and increase `temperature`. Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "izDuZhKxyfbO",
        "outputId": "2e929d00-14ca-4ba1-e2f2-34a93a2b91c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hi, I'm a chatbot. I'm doing some research.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    min_length=20,\n",
        "    max_length=256,\n",
        "    temperature=0.0  # set the temperature, default is 1\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Zq8HZZobyfbP",
        "outputId": "f53b505e-5908-43a2-8bed-fad5f1bc04d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm doing some research. I'm doing some research.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "The chatbot's responses in amusing and entertaining way.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    min_length=10,\n",
        "    max_length=512,\n",
        "    temperature=1.0  # set the temperature, default is 1\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAopPjSSyfbP"
      },
      "source": [
        "The second response is far more creative and demonstrates the type of difference we can expect between low `temperature` and high `temperature` generations.\n",
        "\n",
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IXwMv2iVyfbQ",
        "outputId": "a533ba1c-cdf7-4f1e-fb72-ae442a9fadba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Life is a story of a life. It is a story of a life.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative \n",
        "and funny responses to the users questions. \n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    max_length=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn0aVAvNyfbQ"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "I0oVzvyNyfbR",
        "outputId": "aad38dd4-5186-4c2a-eae5-bd4cff35b1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's time to get a watch.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative \n",
        "and funny responses to the users questions. Here are some examples: \n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    min_length=10,\n",
        "    max_length=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROn1RDzryfbR"
      },
      "source": [
        "This is a much better response and the way we did this was by providing a *few* examples that included the example inputs and outputs that we'd expect. We refer to this as _\"few-shot learning\"_.\n",
        "\n",
        "## Adding Multiple Contexts\n",
        "\n",
        "In some use-cases like question-answering we can use an external source of information to improve the reliability or *factfulness* of model responses. We refer to this information as _\"source knowledge\"_, which is any knowledge fed into the model via the input prompt.\n",
        "\n",
        "We'll create a list of \"dummy\" external information. In reality we'd likely use [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) or some form of information grabbing APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "495o9n1IyfbS"
      },
      "outputs": [],
      "source": [
        "contexts = [\n",
        "    (\n",
        "        \"Large Language Models (LLMs) are the latest models used in NLP. \" +\n",
        "        \"Their superior performance over smaller models has made them incredibly \" +\n",
        "        \"useful for developers building NLP enabled applications. These models \" +\n",
        "        \"can be accessed via Hugging Face's `transformers` library, via OpenAI \" +\n",
        "        \"using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    ),\n",
        "    (\n",
        "        \"To use OpenAI's GPT-3 model for completion (generation) tasks, you \" +\n",
        "        \"first need to get an API key from \" +\n",
        "        \"'https://beta.openai.com/account/api-keys'.\"\n",
        "    ),\n",
        "    (\n",
        "        \"OpenAI's API is accessible via Python using the `openai` library. \" +\n",
        "        \"After installing the library with pip you can use it as follows: \\n\" +\n",
        "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
        "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
        "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
        "    ),\n",
        "    (\n",
        "        \"The OpenAI endpoint is available for completion tasks via the \" +\n",
        "        \"LangChain library. To use it, first install the library with \" +\n",
        "        \"`pip install langchain openai`. Then, import the library and \" +\n",
        "        \"initialize the model as follows: \\n\" +\n",
        "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
        "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
        "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijEGlUQlyfbS"
      },
      "source": [
        "We would feed this external information into our prompt between the initial *instructions* and the *user input*. For OpenAI models it's recommended to separate the contexts from the rest of the prompt using `###` or `\"\"\"`, and each independent context can be separated with a few newlines and `##`, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "lHhaZl7-yfbT",
        "outputId": "c02e0176-7495-4f5e-829b-b047fb93f2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the contexts below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "###\n",
            "\n",
            "Contexts:\n",
            "Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "##\n",
            "\n",
            "To use OpenAI's GPT-3 model for completion (generation) tasks, you first need to get an API key from 'https://beta.openai.com/account/api-keys'.\n",
            "\n",
            "##\n",
            "\n",
            "OpenAI's API is accessible via Python using the `openai` library. After installing the library with pip you can use it as follows: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = \n",
            "'<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "##\n",
            "\n",
            "The OpenAI endpoint is available for completion tasks via the LangChain library. To use it, first install the library with `pip install langchain openai`. Then, import the library and initialize the model as follows: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n",
            "\n",
            "###\n",
            "\n",
            "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
            "using Python from start to finish\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
        "\n",
        "print(f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "75dyV6YiyfbU",
        "outputId": "1d15253e-2dd5-4836-f8ec-94c16e169f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI's GPT-3 model for completion (generation) tasks\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    min_length=10,\n",
        "    max_length=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNThRpx1yfbU"
      },
      "source": [
        "Not bad, but are these contexts actually helping? Maybe the model is able to answer these questions without the additional information (source knowledge) as is able to rely solely on information stored within the model's internal parameters (parametric knowledge). Let's ask again without the external information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fk7YdK0SyfbU",
        "outputId": "a0570226-bf93-4a24-8031-64e829eee578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know. I'm using OpenAI's GPT-3 model using Python from start to finish\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = completion(\n",
        "    prompt=prompt,\n",
        "    min_length=10,\n",
        "    max_length=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VEpKH2abyfbV"
      },
      "source": [
        "These are not really what we asked for, and are definitely not very specific. So clearly adding some source knowledge to our prompts can result in some much better results.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
